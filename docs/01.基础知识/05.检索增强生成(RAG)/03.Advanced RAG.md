---
title: 3. Advanced RAG
date: 2025-09-18 22:59:17
permalink: /rag-advanced/
categories:
  - 基础知识
  - RAG
tags:
  - RAG
author: 
  name: xiao_sl
  link: https://github.com/xiaosl-cell
---

Advanced RAG (高级检索增强生成)并不是一个具有标准定义的术语，它泛指在基础RAG架构之上，为应对特定挑战而引入一系列更复杂、精细且智能的技术与策略所形成的优化RAG集合体。本文将遵循RAG的工作流程，详细剖析这些优化策略。下图（源自 [huggingface](https://huggingface.co/learn/cookbook/en/advanced_rag)）以蓝色标注了RAG系统的各项潜在优化方向。

![Advanced RAG](https://qiniu.agiadventurer.com/advanced_rag.png)

## 3.1 知识库构建阶段

知识库构建阶段是整个RAG系统的基石，其优化策略直接影响到后续的检索效果。原始数据通常是非结构化的，需要经过一系列精细处理才能转化为可供模型高效检索的格式。高级RAG在此阶段的优化主要集中在 分块优化 (Chunking Optimization)、元数据丰富 (Metadata Enrichment) 和 索引结构优化 (Indexing Optimization) 三个方面。

### 3.1.1 分块优化 (Chunk Optimization)

#### 3.1.1.1 重叠分块

为了解决信息跨块分布的问题，我们可以采用重叠块策略，确保重要信息不会因为分块边界而丢失。

```python
def create_overlapping_chunks(text, chunk_size=512, overlap_size=50):
    """
    创建重叠的文本块
    """
    chunks = []
    start = 0
    
    while start < len(text):
        end = start + chunk_size
        chunk = text[start:end]
        chunks.append(chunk)
        
        # 下一个块的起始位置重叠
        start = start + chunk_size - overlap_size
        
        if start >= len(text):
            break
            
    return chunks
```
#### 3.1.1.2 递归分割

对于多级文档，我们可以使用 Langchain 提供的 RecursiveCharacterTextSplitter。RecursiveCharacterTextSplitter 采用递归分割策略，按照预定义的分隔符优先级顺序进行文档分割，确保在保持语义完整性的同时控制分块大小，以下是一个简单的示例：

```bash
pip install langchain
```

```python
from langchain.text_splitter import RecursiveCharacterTextSplitter

default_separators = [
    "\n## ",          # 二级标题
    "\n### ",         # 三级标题
    "\n#### ",        # 四级标题
    "\n\n",           # 段落分隔
    "\n",             # 行分隔
    " ",              # 空格
    ""                # 字符级
]


def create_recursive_splitter(chunk_size=1000, chunk_overlap=200):
    """
    创建递归字符文本分割器
    """
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        separators=default_separators,
        length_function=len,
    )
    return text_splitter

text_splitter = create_recursive_splitter()
chunks = text_splitter.split_text(document_text)
```

#### 3.1.1.3 语义分割

在实际应用中，由于预定义规则很死板，基于规则的分块方法很容易导致诸如检索上下文不完整或分块过大含有噪声等问题。因此最优雅的方法是基于语义进行分块。语义分块的宗旨是确保每个分块尽可能包含语义上独立的信息。

##### 3.1.1.3.1 **SemanticChunker语义分割**

LangChain 提供了 `SemanticChunker` 工具来实现语义分块, 以下是使用示例：

```bash
pip install langchain-experimental langchain-openai
```

```python
from langchain_experimental.text_splitter import SemanticChunker
from langchain_openai import OpenAIEmbeddings

# 初始化嵌入模型
embeddings = OpenAIEmbeddings()

# 创建语义分块器
semantic_chunker = SemanticChunker(
    embeddings=embeddings,
    buffer_size=3, 
    breakpoint_threshold_type="percentile", 
    breakpoint_threshold_amount=70,
)

# 执行语义分块
chunks = semantic_chunker.split_text(document_text)

# 或者直接分割文档
documents = semantic_chunker.split_documents(doc_list)
```

##### 3.1.1.3.2 **SemanticChunker 类的定义**

```python
class SemanticChunker(
    embeddings: Embeddings, # 向量模型，用于生成句子Embedding
    buffer_size: int = 1, # 向前向后取 buffer_size 个句子一起 Embedding，以减少噪声
    add_start_index: bool = False, # 是否在元数据中添加切分块在原文中的起始字符位置
    breakpoint_threshold_type: BreakpointThresholdType = "percentile", # 切分点计算方法，默认为“分位法”
    breakpoint_threshold_amount: float | None = None, # 切分点计算阈值，具体含义取决于 breakpoint_threshold_type
    number_of_chunks: int | None = None, # 期望切分后的文档块数量，用于反向推导阈值
    sentence_split_regex: str = r"(?<=[.?!])\s+", # 句子切分规则，默认为英文标点符号
    min_chunk_size: int | None = None # 最小文档块大小，用于合并过小的文档块
)
```

##### 3.1.1.3.2 **SemanticChunker 基本原理**

- **句子切分**
文档首先需要被切分成独立的句子，SemanticChunker 通过 sentence_split_regex 参数来设置句子切分规则。默认值为 `r"(?<=[.?!])\s+"`，该切分方式适用于英文文档，它以英文的句号、问号、感叹号后跟空白字符作为分隔符。对于中文文档，我们需要将正则表达式替换为能够有效切分中文句子的规则，例如 `r"(?<=[。？！\n])"`，即以中文的句号、问号、感叹号以及换行符作为切分点。

```python
import re

def _get_single_sentences_list(self, text: str) -> List[str]:
    return re.split(self.sentence_split_regex, text)
```

- **句子组合与向量化**
  
在获得独立的句子列表后，SemanticChunker 会根据 `buffer_size` 参数将相邻句子组合成句子组。这种设计的核心目的是通过引入上下文信息来降低单句向量化时的噪声干扰，提升语义表征的准确性。

```python
def combine_sentences(sentences: List[dict], buffer_size: int = 1) -> List[dict]:
    for i in range(len(sentences)):
        combined_sentence = ""
        # 添加当前句子之前 buffer_size 个句子
        for j in range(i - buffer_size, i):
            if j >= 0:
                combined_sentence += sentences[j]["sentence"] + " "
        # 添加当前句子
        combined_sentence += sentences[i]["sentence"]
        # 添加当前句子之后 buffer_size 个句子
        for j in range(i + 1, i + 1 + buffer_size):
            if j < len(sentences):
                combined_sentence += " " + sentences[j]["sentence"]
        sentences[i]["combined_sentence"] = combined_sentence
    return sentences
```

随后，利用嵌入模型将每个句子组转换为高维语义向量，这些向量能够在语义空间中精确表征文本的深层含义。

```python
def _calculate_sentence_distances(self, single_sentences_list: List[str]) -> Tuple[List[float], List[dict]]:
    _sentences = [
        {"sentence": x, "index": i} for i, x in enumerate(single_sentences_list)
    ]
    sentences = combine_sentences(_sentences, self.buffer_size)
    embeddings = self.embeddings.embed_documents(
        [x["combined_sentence"] for x in sentences]
    )
    for i, sentence in enumerate(sentences):
        sentence["combined_sentence_embedding"] = embeddings[i]
    return calculate_cosine_distances(sentences)
```

- **相似度计算与分割点识别**

SemanticChunker 通过计算相邻句子组向量之间的余弦距离来量化语义变化程度。当相邻句子组的语义距离超过预设阈值时，说明话题或语义发生了显著转换，此处即被标记为潜在的分割点。

```python
from langchain_community.utils.math import cosine_similarity

def calculate_cosine_distances(sentences: List[dict]) -> Tuple[List[float], List[dict]]:
    distances = []
    for i in range(len(sentences) - 1):
        embedding_current = sentences[i]["combined_sentence_embedding"]
        embedding_next = sentences[i + 1]["combined_sentence_embedding"]

        similarity = cosine_similarity([embedding_current], [embedding_next])[0][0]
        distance = 1 - similarity
        distances.append(distance)
        sentences[i]["distance_to_next"] = distance
    return distances, sentences
```

距离阈值的确定方式由 `breakpoint_threshold_type` 参数控制，支持三种策略：

1. **百分位法 (percentile)**：基于距离分布的统计特性，选取第 N 百分位数作为阈值。例如设置 `breakpoint_threshold_amount=70`，则将所有距离值的第70百分位数作为切分阈值，超过该值的位置即为分割点。也可以通过 number_of_chunks 参数指定期望的文档块数量，系统会反向推导出相应的分位数。
```python
import numpy as np
def _calculate_breakpoint_threshold(self, distances: List[float]) -> Tuple[float, List[float]]:
    # 第一种方式：指定分位数
    return cast(
        float,
        np.percentile(distances, self.breakpoint_threshold_amount),
    ), distances
```

2. **标准差法 (standard_deviation)**：将所有余弦距离的平均值加上 X 倍的标准差作为阈值。该方法适用于数据呈正态分布的情况，breakpoint_threshold_amount 默认为 3。

```python
import numpy as np
def _calculate_breakpoint_threshold(self, distances: List[float]) -> Tuple[float, List[float]]:
    return cast(
        float,
        np.mean(distances) +
          self.breakpoint_threshold_amount * np.std(distances),),
        distances
```

3. **四分位距法 (interquartile)**：基于四分位距（IQR）检测离群值，阈值计算公式为 `Q3 + k * IQR`，其中 Q3 为第三四分位数，IQR = Q3 - Q1。

```python
import numpy as np
def _calculate_breakpoint_threshold(self, distances: List[float]) -> Tuple[float, List[dict]]:
    q1, q3 = np.percentile(distances, [25, 75])
    iqr = q3 - q1
    return np.mean(distances) + \
           self.breakpoint_threshold_amount * iqr, distances
```
4. **梯度法(gradient)**: 首先计算所有余弦距离的变化梯度，然后将变化梯度的第 X 百分位数作为阈值。这种方法能够捕捉余弦距离变化最快的点，breakpoint_threshold_amount 默认为 95。

```python
import numpy as np
def _calculate_breakpoint_threshold(self, distances: List[float]) -> Tuple[float, List[float]]:
    distance_gradient = np.gradient(distances, range(0, len(distances)))
    return cast(
        float,
        np.percentile(distance_gradient,
                      self.breakpoint_threshold_amount)),
        distance_gradient
```

- **分块生成与后处理**

识别出所有分割点后，SemanticChunker 将原文本按照这些分割点切分成多个语义完整的文本块。为了避免产生过小的碎片化分块，可以通过 `min_chunk_size` 参数设定最小块大小约束，将过小的分块与相邻块合并。

```python
def split_text(self, text: str,) -> List[str]:
    distances, sentences = self._calculate_sentence_distances(single_sentences_list)
    breakpoint_distance_threshold, breakpoint_array = self._calculate_breakpoint_threshold(distances)

    indices_above_thresh = [
        i
        for i, x in enumerate(breakpoint_array)
        if x > breakpoint_distance_threshold
    ]

    chunks = []
    start_index = 0

    for index in indices_above_thresh:
        end_index = index
        group = sentences[start_index : end_index + 1]
        combined_text = " ".join([d["sentence"] for d in group])

        if (
            self.min_chunk_size is not None
            and len(combined_text) < self.min_chunk_size
        ):
            continue
        chunks.append(combined_text)
        start_index = index + 1

    if start_index < len(sentences):
        combined_text = " ".join([d["sentence"] for d in sentences[start_index:]])
        chunks.append(combined_text)
    return chunks
```

经过 SemanticChunker 切分后，虽然语义连贯性得到了提升，但切分出的文档块可能存在长度不均的问题。为了获得更好的检索效果，我们通常需要对切分后的文档块进行二次处理，比如二次切分较长的文档块，合并较短的文档块，添加标题等


### 3.1.2 元数据丰富 (Metadata Enrichment)
