---
title: 3. Ingestion Pipeline
date: 2025-12-03 16:05:00
permalink: /llamaindex-pipeline/
sidebar: true
article: true
author: 
  name: xiao_sl
  link: https://github.com/xiaosl-cell
categories: 
  - 基础知识
  - LlamaIndex
tags: 
  - LlamaIndex
  - RAG
---

LlamaIndex 通过 Transformations 定义一个数据（Documents）的多步处理的流程（Pipeline）。Pipeline 的显著特点是，它的每个子步骤是可以缓存（cache）的，即如果该子步骤的输入与处理方法不变，重复调用时会直接从缓存中获取结果，而无需重新执行该子步骤，这样既节省时间也会节省 token（如果子步骤涉及大模型调用）。

```python
import time
import chromadb
from chromadb import Settings
from llama_index.core import VectorStoreIndex
from llama_index.vector_stores.chroma import ChromaVectorStore
from llama_index.core import SimpleDirectoryReader
from llama_index.readers.file import PyMuPDFReader
from llama_index.core.ingestion import IngestionPipeline
from llama_index.embeddings.openai import OpenAIEmbedding
from llama_index.core.node_parser import SentenceSplitter
from llama_index.core.extractors import TitleExtractor


class Timer:
    def __enter__(self):
        self.start = time.time()
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        self.end = time.time()
        self.interval = self.end - self.start
        print(f"耗时 {self.interval*1000} ms")


chroma_client = chromadb.HttpClient(settings=Settings(allow_reset=True), port=8000)
chroma_client.reset()
chroma_collection = chroma_client.get_or_create_collection("llama_index_demo")

vector_store = ChromaVectorStore(chroma_collection=chroma_collection)
pipeline = IngestionPipeline(
    transformations=[
        SentenceSplitter(chunk_size=300, chunk_overlap=100),  # 按句子切分
        TitleExtractor(),  # 利用 LLM 对文本生成标题
        OpenAIEmbedding(),  # 将文本向量化
    ],
    vector_store=vector_store,
)

documents = SimpleDirectoryReader(
    "./data",
    required_exts=[".pdf"],
    file_extractor={".pdf": PyMuPDFReader()}
).load_data()

# 计时
with Timer():
    # Ingest directly into a vector db
    pipeline.run(documents=documents)

# 创建索引
index = VectorStoreIndex.from_vector_store(vector_store)

# 获取 retriever
vector_retriever = index.as_retriever(similarity_top_k=1)

# 检索
results = vector_retriever.retrieve("Llama2有多少参数")

# 本地保存 `IngestionPipeline` 的缓存
pipeline.persist("./pipeline_storage")

new_pipeline = IngestionPipeline(
    transformations=[
        SentenceSplitter(chunk_size=300, chunk_overlap=100),
        TitleExtractor(),
    ],
)

# 加载缓存
new_pipeline.load("./pipeline_storage")

with Timer():
    nodes = new_pipeline.run(documents=documents)
```

可以看到第二次执行耗时大幅减少，证明缓存生效。注意 `new_pipeline` 相较于 `pipeline` 少了 `OpenAIEmbedding()`，但缓存仍然生效。

此外，也可以用远程的 Redis 或 MongoDB 等存储 IngestionPipeline 的缓存，具体参考官方文档：[Remote Cache Management](https://docs.llamaindex.ai/en/stable/module_guides/loading/ingestion_pipeline/#remote-cache-management)。

IngestionPipeline 也支持异步和并发调用，参考官方文档：[Async Support](https://docs.llamaindex.ai/en/stable/module_guides/loading/ingestion_pipeline/#async-support)、[Parallel Processing](https://docs.llamaindex.ai/en/stable/module_guides/loading/ingestion_pipeline/#parallel-processing)。
